{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA TOPIC MODELING \n",
    "\n",
    "This notebook applies LDA modeling to a dataset and used GENSIM toolkit:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Steps__\n",
    "\n",
    "- Text porcessing\n",
    "- Bulding the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Referred links:__\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\n",
    "\n",
    "- https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/\n",
    "\n",
    "- https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d\n",
    "\n",
    "- https://stackoverflow.com/questions/20984841/topic-distribution-how-do-we-see-which-document-belong-to-which-topic-after-doi\n",
    "\n",
    "- https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Required Libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import string\n",
    "import re\n",
    "from pprint import pprint\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# NLTK \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english') #this depends on each language\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "import html\n",
    "\n",
    "import ast\n",
    "\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Loading data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./listing_merged2.csv\", index_col = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[['listing_id', 'description', 'title', 'tags', 'url', 'img1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def escape(texts): \n",
    "    return [html.unescape(str(text)) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.description = data.description.apply(lambda x: html.unescape(x))\n",
    "data.title = data.title.apply(lambda x: html.unescape(x))\n",
    "data.tags = data.tags.apply(lambda x: ast.literal_eval(x))                                                                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten list\n",
    "data.tags = [','.join(x) for x in data.tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tags = data.tags.apply(lambda x: html.unescape(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = data.tags.apply(lambda x: len(x) > 3)\n",
    "data = data.loc[mask, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove not filtered german\n",
    "mask1 = data.tags.apply(lambda x: \"ROSENKNOPF,PUPPENKÃœCHE\" not in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[mask1, ] #fine remove german"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = data['description'].values.tolist()\n",
    "tl = data['title'].values.tolist()\n",
    "tag = data['tags'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove url\n",
    "def remove_html(texts):\n",
    "    return [ re.sub(r\"https?:\\S+\", \"\", str(text)) for text in texts]\n",
    "\n",
    "ds = remove_html(ds)\n",
    "tl = remove_html(tl)\n",
    "tag = remove_html(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(ds[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tokenize words and Clean-up text__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "ds_words = list(sent_to_words(ds))\n",
    "tl_words = list(sent_to_words(tl))\n",
    "tag_words = list(sent_to_words(tag))\n",
    "print(ds_words[:1])\n",
    "print(tl_words[:1])\n",
    "print(tag_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['toy', 'children', 'child', 'kid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Process Text__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_words(data_words):   \n",
    "    # Bigrams are two words frequently occurring together in the document. Trigrams are 3 words frequently occurring.\n",
    "    # Build the bigram and trigram models\n",
    "    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "    # See trigram example\n",
    "    print(trigram_mod[bigram_mod[data_words[0]]])\n",
    "\n",
    "\n",
    "    #Remove Stopwords, Make Bigrams and Lemmatize\n",
    "    # Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "    \n",
    "\n",
    "    def remove_stopwords(texts):\n",
    "        return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "    def make_bigrams(texts):\n",
    "        return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "    def make_trigrams(texts):\n",
    "        return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "    def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "        \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "        texts_out = []\n",
    "        for sent in texts:\n",
    "            doc = nlp(\" \".join(sent)) \n",
    "            texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "        return texts_out\n",
    "    \n",
    "    # Remove Stop Words\n",
    "    data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "    # Form Bigrams\n",
    "    data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "    # Initialize spacy 'es' model, keeping only tagger component (for efficiency)\n",
    "    # python3 -m spacy download es\n",
    "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "    # Do lemmatization keeping only noun, adj, vb, adv\n",
    "    data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "    # Create Corpus\n",
    "    texts = data_lemmatized\n",
    "\n",
    "    # Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    \n",
    "    return data_lemmatized, corpus, id2word\n",
    "\n",
    "ds_data_lemmatized, ds_corpus, ds_id2word = process_words(ds_words)\n",
    "tl_data_lemmatized, tl_corpus, tl_id2word = process_words(tl_words)\n",
    "tag_data_lemmatized, tag_corpus, tag_id2word = process_words(tag_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create the Dictionary and Corpus needed for Topic Modeling__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to see what word a given id corresponds to, pass the id as a key to the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, you can see a human-readable form of the corpus itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Building the Topic Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimate of number of topics\n",
    "from gensim.models import HdpModel\n",
    "def hdp_topic(corpus, id2word):\n",
    "    hdp = HdpModel(corpus, id2word, chunksize=10000)\n",
    "    return hdp\n",
    "hdp_ds = hdp_topic(ds_corpus, ds_id2word)\n",
    "hdp_tl = hdp_topic(tl_corpus, tl_id2word)\n",
    "hdp_tag = hdp_topic(tag_corpus, tag_id2word)\n",
    "print(\"ds:\", len(hdp_ds.print_topics()))\n",
    "print(\"tl:\", len(hdp_tl.print_topics()))\n",
    "print(\"tag:\", len(hdp_tag.print_topics()))\n",
    "# hdp.print_topics(num_topics=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hdp.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdp.print_topics(num_topics=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "ds_lda_model = gensim.models.LdaMulticore(ds_corpus, num_topics=3, id2word=ds_id2word, eval_every =1, passes=4, workers=5)\n",
    "tl_lda_model = gensim.models.LdaMulticore(tl_corpus, num_topics=9, id2word=tl_id2word, eval_every = 1, passes=2, workers=6)\n",
    "tag_lda_model = gensim.models.LdaMulticore(tag_corpus, num_topics = 2, id2word=tag_id2word, eval_every=1, passes=2, workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pprint(ds_lda_model.print_topics())\n",
    "pprint(tag_lda_model.print_topics())\n",
    "pprint(tl_lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(ds_lda_model.print_topics())\n",
    "doc_lda = ds_lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run tests\n",
    "test = [['toddler', 'color'], ['free', 'shipping']]\n",
    "from gensim.corpora import Dictionary\n",
    "corpus_test = [ds_id2word.doc2bow(text) for text in test]\n",
    "for n in range(len(corpus_test)):\n",
    "    row = ds_lda_model[corpus_test[n]]\n",
    "    row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(ds_lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__View the topics in LDA model__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Compute Model Perplexity and Coherence Score__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Visualize the topics-keywords__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=tl_lda_model, corpus=tl_corpus, texts=tl):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format\n",
    "df_topic_sents_keywords_ds = format_topics_sentences(ldamodel = ds_lda_model, corpus=ds_corpus, texts=ds)\n",
    "df_dominant_topic_ds = df_topic_sents_keywords_ds.reset_index()\n",
    "df_dominant_topic_ds.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic_ds.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Format\n",
    "df_topic_sents_keywords_tag = format_topics_sentences(ldamodel = tag_lda_model, corpus=tag_corpus, texts=tag)\n",
    "df_dominant_topic_tag = df_topic_sents_keywords_tag.reset_index()\n",
    "df_dominant_topic_tag.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic_tag.iloc[21:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Format\n",
    "df_topic_sents_keywords_tl = format_topics_sentences(ldamodel = tl_lda_model, corpus=tl_corpus, texts=tl)\n",
    "df_dominant_topic_tl = df_topic_sents_keywords_tl.reset_index()\n",
    "df_dominant_topic_tl.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic_tl.head(10)#.iloc[21:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add to data\n",
    "data['dmt_ds'] = df_dominant_topic_ds['Dominant_Topic']\n",
    "data['tpc_ds'] = df_dominant_topic_ds['Topic_Perc_Contrib']\n",
    "data['kw_ds'] = df_dominant_topic_ds['Keywords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to data\n",
    "data['dmt_tag'] = df_dominant_topic_tag['Dominant_Topic']\n",
    "data['tpc_tag'] = df_dominant_topic_tag['Topic_Perc_Contrib']\n",
    "data['kw_tag'] = df_dominant_topic_tag['Keywords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic_tag.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('result.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Find the most representative document for each topic__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords_tag.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(3)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Tags\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet#.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Topic distribution across documents__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Documents for Each Topic\n",
    "topic_counts = df_topic_sents_keywords_tag['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Percentage of Documents for Each Topic\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# Topic Number and Keywords\n",
    "topic_num_keywords = df_topic_sents_keywords_tag[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "# Concatenate Column wise\n",
    "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "\n",
    "# Change Column names\n",
    "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "# Show\n",
    "df_dominant_topics.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test = [['busy', 'engage', 'gift', 'puzzle']]\n",
    "\n",
    "id2word_test = corpora.Dictionary(test)\n",
    "\n",
    "# Create Corpus\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus_test = [id2word_test.doc2bow(text) for text in test]\n",
    "\n",
    "print(corpus_test[0])\n",
    "topics = sorted(optimal_model[corpus_test],\n",
    "                key=lambda \n",
    "                x:x[1],\n",
    "                reverse=True)\n",
    "print(topics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
